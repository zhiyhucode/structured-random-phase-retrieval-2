from __future__ import annotations

from deepinv.utils.demo import load_url_image, get_image_url
from matplotlib.patches import Patch
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch


def init_with(x_init):
    def func(y, physics):
        return {"est": (x_init, x_init)}

    return func


def generate_signal(
    shape,
    mode: tuple[str, str] = ("unit", "shepp-logan"),
    transform=None,
    config: dict | None = None,
    phase_range=(-torch.pi, torch.pi),
    dtype=torch.complex64,
    device="cpu",
):
    if config is None:
        config = {}

    if len(mode) == 1:
        if mode[0] == "adversarial":
            x = config["physics"].get_adversarial(n_layers=config["n_layers"])
        else:
            raise ValueError("Invalid mode.")

    if len(mode) == 2:
        if mode[0] == "unit":
            mag = torch.ones(shape, device=device)
        elif mode[0] == "random":
            # * the magnitude response of a signal with unit magnitudes and uniform phase is Rayleigh distributed
            mag = (
                torch.tensor(np.random.rayleigh(scale=1, size=shape))
                .to(dtype)
                .to(device)
            )
        elif mode[0] == "delta":
            mag = torch.zeros(shape, device=device)
            center = tuple(x // 2 for x in shape)
            mag[center] = 1
        else:
            raise ValueError("Invalid magnitude mode.")

        if mode[1] == "shepp-logan":
            url = get_image_url("SheppLogan.png")
            phase = load_url_image(
                url=url,
                img_size=shape[-1],
                grayscale=True,
                resize_mode="resize",
                device=device,
            )
        elif mode[1] == "random":
            # random phase signal
            phase = torch.rand(shape, device=device)
        elif mode[1] == "delta":
            phase = torch.zeros(shape, device=device)
            # select the middle element and set it to 1
            idx = tuple(x // 2 for x in shape)
            phase[idx] = 1
        elif mode[1] == "constant":
            phase = torch.zeros(shape, dtype=dtype, device=device)
        elif mode[1] == "polar":
            # Create a tensor of probabilities (0.5 for each element)
            probabilities = torch.full(shape, 0.5)
            # Generate a tensor with values 0 or 1, with a 50% chance for each
            phase = torch.bernoulli(probabilities)
        else:
            raise ValueError("Invalid mode.")

        if transform:
            if transform == "reverse":
                phase = 1 - phase
            elif transform == "permute":

                def permute(arr: torch.Tensor) -> torch.Tensor:
                    # Step 1: Create a permutation for values in range [0, 255]
                    permuted_values = np.random.permutation(256)

                    # Step 2: Create a mapping from original values to permuted values
                    value_mapping = {i: permuted_values[i] for i in range(256)}

                    # Step 3: Apply the mapping to the original array
                    permuted_array = np.vectorize(value_mapping.get)(arr.cpu() * 255)

                    return torch.from_numpy(permuted_array) / 255

                phase = permute(phase)
            elif transform == "noise":
                phase = (
                    phase * (1 - config["noise_ratio"])
                    + torch.rand_like(phase) * config["noise_ratio"]
                )
            else:
                raise ValueError("Invalid transform.")

        # generate phase signal
        x = mag * torch.exp(
            1j * phase * (phase_range[1] - phase_range[0]) + 1j * phase_range[0]
        ).to(dtype).to(device)

    return x


def default_preprocessing(y, physics):
    r"""
    Default preprocessing function for spectral methods.

    The output of the preprocessing function is given by:

    .. math::
        \max(1 - 1/y, -5).

    :param torch.Tensor y: Measurements.
    :param deepinv.physics.Physics physics: Instance of the physics modeling the forward matrix.

    :return: The preprocessing function values evaluated at y.
    """
    return torch.max(1 - 1 / y, torch.tensor(-5.0))


def correct_global_phase(
    x_recon: torch.Tensor,
    x: torch.Tensor,
    threshold: float = 1e-5,
    verbose: bool = False,
) -> torch.Tensor:
    r"""
        Corrects the global phase of the reconstructed image.

    .. warning::

        Do not mix the order of the reconstructed and original images since this function modifies x_recon in place.


        The global phase shift is computed per image and per channel as:

        .. math::
            e^{-i \phi} = \frac{\conj{\hat{x}} \cdot x}{|x|^2},

        where :math:`\conj{\hat{x}}` is the complex conjugate of the reconstructed image, :math:`x` is the reference image, and :math:`|x|^2` is the squared magnitude of the reference image.

        The global phase shift is then applied to the reconstructed image as:

        .. math::
            \hat{x} = \hat{x} \cdot e^{-i \phi},

        for the corresponding image and channel.

        :param torch.Tensor x_recon: Reconstructed image.
        :param torch.Tensor x: Original image.
        :param float threshold: Threshold to determine if the global phase shift is constant. Default is 1e-5.
        :param bool verbose: If True, prints information about the global phase shift. Default is False.

        :return: The corrected image.
    """
    assert x_recon.shape == x.shape, "The shapes of the images should be the same."
    assert len(x_recon.shape) == 4, (
        "The images should be input with shape (N, C, H, W) "
    )

    n_imgs = x_recon.shape[0]
    n_channels = x_recon.shape[1]

    for i in range(n_imgs):
        for j in range(n_channels):
            e_minus_phi = (x_recon[i, j].conj() * x[i, j]) / (x[i, j].abs() ** 2)
            if e_minus_phi.var() < threshold:
                if verbose:
                    print(f"Image {i}, channel {j} has a constant global phase shift.")
            else:
                if verbose:
                    print(f"Image {i}, channel {j} does not have a global phase shift.")
            e_minus_phi = e_minus_phi.mean()
            x_recon[i, j] = x_recon[i, j] * e_minus_phi

    return x_recon


def cosine_similarity(a: torch.Tensor, b: torch.Tensor):
    r"""
    Compute the cosine similarity between two images.

    The cosine similarity is computed as:

    .. math::
        \text{cosine\_similarity} = \frac{a \cdot b}{\|a\| \cdot \|b\|}.

    The value range is [0,1], higher values indicate higher similarity.
    If one image is a scaled version of the other, i.e., :math:`a = c * b` where :math:`c` is a nonzero complex number, then the cosine similarity will be 1.

    :param torch.Tensor a: First image.
    :param torch.Tensor b: Second image.
    :return: The cosine similarity between the two images."""
    assert a.shape == b.shape
    a = a.flatten()
    b = b.flatten()
    norm_a = torch.sqrt(torch.dot(a.conj(), a).real)
    norm_b = torch.sqrt(torch.dot(b.conj(), b).real)
    return torch.abs(torch.dot(a.conj(), b)) / (norm_a * norm_b)


def compute_lipschitz_constant(
    x_est: torch.Tensor,
    y: torch.Tensor,
    physics,
    spectrum: str,
    loss: str,
):
    r"""
    Compute the lipschitz constant of the gradient of a loss function for random phase retrieval.

    :param torch.Tensor x_est: Estimated measurements.
    :param torch.Tensor y: True measurements.
    :param deepinv.physics.PhaseRetrieval physics: Instance of the physics.
    :param str spectrum: Spectrum of the forward matrix. Can be 'marchenko' or 'unitary'.
    :param str loss: loss function. Can be 'intensity' or 'amplitude'.
    """
    # compute maximum eigenvalue of A^H@A
    if spectrum == "marchenko":
        lambda_max = (
            1 + np.sqrt(1 / physics.oversampling_ratio)
        ) ** 2  # https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution
    elif spectrum == "unitary":
        lambda_max = 1
    else:
        raise ValueError(f"Unsupported spectrum: {spectrum}")

    if loss == "intensity":
        diag_max = (2 * physics(x_est) - y).abs().max()
    elif loss == "amplitude":
        diag_max = (1 - 0.5 * torch.sqrt(y) / torch.sqrt(physics(x_est))).abs().max()
    else:
        raise ValueError(f"Unsupported loss: {loss}")

    return 2 * lambda_max * diag_max


def spectral_methods(
    y: torch.Tensor,
    physics,
    x=None,
    n_iter=500,
    preprocessing=default_preprocessing,
    lamb=10.0,
    x_true=None,
    log: bool = False,
    log_metric=cosine_similarity,
    early_stop: bool = True,
    rtol: float = 1e-5,
    verbose: bool = False,
):
    r"""
    Utility function for spectral methods.

    This function runs the Spectral Methods algorithm to find the principal eigenvector of the regularized weighted covariance matrix:
    
    .. math::
        \begin{equation*}
        M = \conj{B} \text{diag}(T(y)) B + \lambda I,
        \end{equation*}
    
    where :math:`B` is the linear operator of the phase retrieval class, :math:`T(\cdot)` is a preprocessing function for the measurements, and :math:`I` is the identity matrix of corresponding dimensions. Parameter :math:`\lambda` tunes the strength of regularization.

    To find the principal eigenvector, the function runs power iteration which is given by

    .. math::
        \begin{equation*}
        \begin{aligned}
        x_{k+1} &= M x_k \\
        x_{k+1} &= \frac{x_{k+1}}{\|x_{k+1}\|},
        \end{aligned}
        \end{equation*}
  
    :param torch.Tensor y: Measurements.
    :param deepinv.physics.Physics physics: Instance of the physics modeling the forward matrix.
    :param torch.Tensor x: Initial guess for the signals :math:`x_0`.
    :param int n_iter: Number of iterations.
    :param Callable preprocessing: Function to preprocess the measurements. Default is :math:`\max(1 - 1/x, -5)`.
    :param float lamb: Regularization parameter. Default is 10.
    :param bool log: Whether to log the metrics. Default is False.
    :param Callable log_metric: Metric to log. Default is cosine similarity.
    :param bool early_stop: Whether to early stop the iterations. Default is True.
    :param float rtol: Relative tolerance for early stopping. Default is 1e-5.
    :param bool verbose: If True, prints information in case of an early stop. Default is False.

    :return: The estimated signals :math:`x`.
    """
    if x is None:
        # always use randn for initial guess, never use rand!
        x = torch.randn(
            (y.shape[0],) + physics.img_size,
            dtype=physics.dtype,
            device=physics.device,
        )

    if log is True:
        metrics = []

    #! estimate the norm of x using y
    #! for the i.i.d. case, we have norm(x) = sqrt(sum(y)/A_squared_mean)
    #! for the structured case, when the mean of the squared diagonal elements is 1, we have norm(x) = sqrt(sum(y)), otherwise y gets scaled by the mean to the power of number of layers
    norm_x = torch.sqrt(y.sum())

    x = x.to(torch.cfloat)
    # y should have mean 1
    y = y / torch.mean(y)
    diag_T = preprocessing(y, physics)
    diag_T = diag_T.to(torch.cfloat)
    for i in range(n_iter):
        x_new = physics.B(x)
        x_new = diag_T * x_new
        x_new = physics.B_adjoint(x_new)
        x_new = x_new + lamb * x
        x_new = x_new / torch.linalg.norm(x_new)
        if log:
            metrics.append(log_metric(x_new, x_true))
        if early_stop:
            if torch.linalg.norm(x_new - x) / torch.linalg.norm(x) < rtol:
                if verbose:
                    print(f"Power iteration early stopped at iteration {i}.")
                break
        x = x_new
    #! change the norm of x so that it matches the norm of true x
    x = x * norm_x
    if log:
        return x, metrics
    else:
        return x


def spectral_methods_wrapper(y, physics, n_iter=5000, **kwargs):
    x = spectral_methods(y, physics, n_iter=n_iter, log=False, **kwargs)
    z = x.detach().clone()
    return {"est": (x, z)}


def plot_error_bars(
    oversamplings,
    datasets,
    labels,
    xlim=None,
    xticks=None,
    ylim=None,
    yticks=None,
    axis=1,
    title: str | None = None,
    xlabel="Oversampling Ratio",
    ylabel="Cosine Similarity",
    xscale="linear",
    yscale="linear",
    save_dir: str | None = None,
    figsize=(10, 6),
    marker=".",
    markersize=10,
    capsize=5,
    font="Times New Roman",
    fontsize=14,
    labelsize=16,
    ticksize=16,
    colormap="viridis",
    error_bar="quantile",
    quantiles: list[float] | None = None,
    error_bar_linestyle="--",
    error_bars=True,
    plot="other",
    legend_loc="upper left",
    transparent=True,
    show=True,
    bbox_inches="tight",
):
    if quantiles is None:
        quantiles = [0.10, 0.50, 0.90]
    # Generate a color palette
    palette = sns.color_palette(n_colors=len(datasets))

    plt.rcParams["font.family"] = font
    plt.rcParams["font.size"] = fontsize
    plt.rcParams["axes.labelsize"] = labelsize
    # plt.rcParams["text.usetex"] = True if shutil.which("latex") else False
    plt.figure(figsize=figsize)

    for i, (oversampling, data, label) in enumerate(
        zip(oversamplings, datasets, labels)
    ):
        data = data.copy()

        if plot == "reconstruction":
            if "Structured" in label:
                color = palette[0]
            elif "Dense" in label:
                color = palette[3]

            if "GD+SM" in label:
                linestyle = "-"
            elif "SM" in label:
                linestyle = (
                    0,
                    (5, 5),
                )  # Custom dashed pattern: 5 points on, 5 points off
            elif "GD" in label:
                linestyle = ":"
            else:
                raise ValueError("Invalid label for reconstruction plot.")
        elif plot == "spectrum":
            if "Structured Marchenko" in label:
                color = palette[2]
            elif "Structured Unitary" in label:
                color = palette[1]
            elif "i.i.d. Gaussian" in label:
                color = palette[0]
            elif "Unitary" in label:
                color = palette[3]

            if "GD+SM" in label:
                linestyle = "-"
            elif "SM" in label:
                linestyle = ":"
            elif "GD" in label:
                linestyle = ":"
            else:
                raise ValueError("Invalid label for spectrum plot.")
        elif plot == "depth":
            gradient_colors = [palette[3], palette[1], palette[2], palette[0]]
            if "1 Layer" in label:
                color = gradient_colors[0]
            elif "1.5 Layers" in label:
                color = gradient_colors[1]
            elif "2 Layers" in label:
                color = gradient_colors[2]
            elif "3 Layers" in label:
                color = gradient_colors[3]
            else:
                raise ValueError("Invalid label for depth plot.")

            if "GD+SM" in label:
                linestyle = "-"
            elif "SM" in label:
                linestyle = ":"
            else:
                raise ValueError("Invalid label for depth plot.")
        elif plot == "tdt":
            gradient_colors = [palette[3], palette[1], palette[2], palette[0]]
            if "Constant" in label:
                color = gradient_colors[0]
            elif "Shepp-Logan" in label:
                color = gradient_colors[1]
            elif "Random 2 Layers" in label:
                color = gradient_colors[3]
            elif "Random" in label:
                color = gradient_colors[2]
            else:
                raise ValueError("Invalid label for depth plot.")

            if "GD+SM" in label:
                linestyle = "-"
            elif "SM" in label:
                linestyle = ":"
            else:
                raise ValueError("Invalid label for depth plot.")
        elif plot == "time":
            if "Structured" in label:
                if "GPU" in label:
                    color = palette[2]
                else:
                    color = palette[0]
            elif "Dense" in label:
                if "GPU" in label:
                    color = palette[1]
                else:
                    color = palette[3]
            linestyle = "-"
        elif plot == "noise":
            if "100" in label:
                color = palette[3]
            elif "75" in label:
                color = palette[1]
            elif "50" in label:
                color = palette[4]
            elif "25" in label:
                color = palette[0]
            elif "0" in label:
                color = palette[2]
            else:
                raise ValueError("Invalid label for noise plot.")
            if "GD" in label:
                linestyle = "-"
            elif "SM" in label:
                linestyle = ":"
            else:
                raise ValueError("Invalid label for depth plot.")
        else:
            color = palette[i]
            if "GD" in label:
                linestyle = ":"
            elif "GD+SM" in label:
                linestyle = "-"
            elif "SM" in label:
                linestyle = ":"
            else:
                linestyle = "-"

        # Calculate statistics
        if type(data) == torch.Tensor:
            std_vals = data.std(dim=1).numpy()
            avg_vals = data.mean(dim=1).numpy()
            min_vals = avg_vals - std_vals
            max_vals = avg_vals + std_vals
        elif type(data) == pd.DataFrame:
            # if plot == "reconstruction" or plot == "layer":
            for column in data.columns:
                if "img_size" in column:
                    pass
                elif "repeat" not in column:
                    data.drop(columns=column, inplace=True)
            if error_bar == "quantile":
                min_vals = data.quantile(q=quantiles[0], axis=axis).values
                avg_vals = data.quantile(q=quantiles[1], axis=axis).values
                max_vals = data.quantile(q=quantiles[2], axis=axis).values
            elif error_bar == "std":
                avg_vals = data.mean(axis=axis).values
                std_vals = data.std(axis=axis).values
                min_vals = avg_vals - std_vals
                max_vals = avg_vals + std_vals

        # Calculate error bars
        yerr_lower = avg_vals - min_vals
        yerr_upper = max_vals - avg_vals

        # Prepare data for plotting
        df = pd.DataFrame(
            {
                "x": oversampling,
                "mid": avg_vals,
                "yerr_lower": yerr_lower,
                "yerr_upper": yerr_upper,
            }
        )

        # Plotting
        ax = sns.lineplot(
            data=df,
            x="x",
            y="mid",
            marker=marker,
            label=label,
            color=color,
            markersize=markersize,
            linestyle=linestyle,
            zorder=2,
        )
        if error_bars:
            # Adding error bars
            eb = ax.errorbar(
                df["x"],
                df["mid"],
                yerr=[df["yerr_lower"], df["yerr_upper"]],
                fmt=marker,
                capsize=capsize,
                color=color,
                zorder=2,
            )
            eb[-1][0].set_linestyle(error_bar_linestyle)

    if plot == "reconstruction":
        legend_contents = [
            (Patch(visible=False), "Model"),
            (
                plt.Line2D([], [], linestyle="-", color=palette[0]),
                "Structured",
            ),
            (plt.Line2D([], [], linestyle="-", color=palette[3]), "Dense"),
            (Patch(visible=False), "Algorithm"),
            (plt.Line2D([], [], linestyle="-", marker=".", color="black"), "GD + SM"),
            (
                plt.Line2D([], [], linestyle=(0, (5, 5)), marker=".", color="black"),
                "SM",
            ),
            (plt.Line2D([], [], linestyle=":", marker=".", color="black"), "GD"),
        ]
        legend = ax.legend(*zip(*legend_contents), loc=legend_loc)
        for text in legend.get_texts():
            if text.get_text() in ["Model", "Algorithm"]:
                text.set_fontweight("bold")
                text.set_fontsize(text.get_fontsize() * 1.05)
    elif plot == "spectrum":
        legend_contents = [
            (Patch(visible=False), "Model"),
            (plt.Line2D([], [], linestyle="-", color=palette[0]), "i.i.d. Gaussian"),
            (plt.Line2D([], [], linestyle="-", color=palette[3]), "Unitary"),
            (
                plt.Line2D([], [], linestyle="-", color=palette[2]),
                "Structured Marchenko",
            ),
            (plt.Line2D([], [], linestyle="-", color=palette[1]), "Structured Unitary"),
            # (Patch(visible=False), ''),  # spacer
            (Patch(visible=False), "Algorithm"),
            (plt.Line2D([], [], linestyle="-", marker=".", color="black"), "GD + SM"),
            (plt.Line2D([], [], linestyle=":", marker=".", color="black"), "SM"),
        ]
        legend = ax.legend(*zip(*legend_contents), loc=legend_loc)
        for text in legend.get_texts():
            if text.get_text() in ["Model", "Algorithm"]:
                text.set_fontweight("bold")
                text.set_fontsize(text.get_fontsize() * 1.05)
    elif plot == "depth":
        # gradient_colors = [colormap(0.1 + i * (0.7-0.1) / 3) for i in range(3,-1,-1)]
        gradient_colors = [palette[3], palette[1], palette[2], palette[0]]

        legend_contents = [
            (Patch(visible=False), "Structure"),
            (plt.Line2D([], [], linestyle="-", color=gradient_colors[0]), "FD"),
            (plt.Line2D([], [], linestyle="-", color=gradient_colors[1]), "FDF"),
            (plt.Line2D([], [], linestyle="-", color=gradient_colors[2]), "FDFD"),
            (plt.Line2D([], [], linestyle="-", color=gradient_colors[3]), "FDFDFD"),
            # (Patch(visible=False), ''),  # spacer
            (Patch(visible=False), "Algorithm"),
            (plt.Line2D([], [], linestyle="-", marker=".", color="black"), "GD + SM"),
            (plt.Line2D([], [], linestyle=":", marker=".", color="black"), "SM"),
        ]
        legend = ax.legend(*zip(*legend_contents), loc=legend_loc)
        for text in legend.get_texts():
            if text.get_text() in ["Algorithm", "Structure"]:
                text.set_fontweight("bold")
                text.set_fontsize(text.get_fontsize() * 1.05)
    elif plot == "tdt":
        # gradient_colors = [colormap(0.1 + i * (0.7-0.1) / 3) for i in range(3,-1,-1)]
        gradient_colors = [palette[3], palette[1], palette[2], palette[0]]

        legend_contents = [
            (Patch(visible=False), "Signal"),
            (plt.Line2D([], [], linestyle="-", color=gradient_colors[0]), "Constant"),
            (
                plt.Line2D([], [], linestyle="-", color=gradient_colors[1]),
                "Shepp-Logan",
            ),
            (plt.Line2D([], [], linestyle="-", color=gradient_colors[2]), "Random"),
            (
                plt.Line2D([], [], linestyle="-", color=gradient_colors[3]),
                "Random 2 Layers",
            ),
            # (Patch(visible=False), ''),  # spacer
            (Patch(visible=False), "Algorithm"),
            (plt.Line2D([], [], linestyle="-", marker=".", color="black"), "GD + SM"),
            (plt.Line2D([], [], linestyle=":", marker=".", color="black"), "SM"),
        ]
        legend = ax.legend(*zip(*legend_contents), loc=legend_loc)
        for text in legend.get_texts():
            if text.get_text() in ["Algorithm", "Signal"]:
                text.set_fontweight("bold")
                text.set_fontsize(text.get_fontsize() * 1.05)
    elif plot == "time":
        legend = ax.legend(loc=legend_loc)
    elif plot == "noise":
        legend_contents = [
            (Patch(visible=False), "Noise Level"),
            (plt.Line2D([], [], linestyle="-", color=palette[3]), "100%"),
            (plt.Line2D([], [], linestyle="-", color=palette[1]), "75%"),
            (plt.Line2D([], [], linestyle="-", color=palette[4]), "50%"),
            (plt.Line2D([], [], linestyle="-", color=palette[0]), "25%"),
            (plt.Line2D([], [], linestyle="-", color=palette[2]), "0%"),
            # (Patch(visible=False), ''),  # spacer
            (Patch(visible=False), "Algorithm"),
            (plt.Line2D([], [], linestyle="-", marker=".", color="black"), "GD + SM"),
            (plt.Line2D([], [], linestyle=":", marker=".", color="black"), "SM"),
        ]
        legend = ax.legend(*zip(*legend_contents), loc=legend_loc)
        for text in legend.get_texts():
            if text.get_text() in ["Algorithm", "Noise Level"]:
                text.set_fontweight("bold")
                text.set_fontsize(text.get_fontsize() * 1.05)
    else:
        legend = ax.legend(loc=legend_loc)
    # set legend on the bottom layer
    legend.set_zorder(1000)

    # Adding labels and title
    ax.set_xlabel(xlabel)
    ax.set_xscale(xscale)
    ax.set_ylabel(ylabel)
    ax.set_yscale(yscale)
    if xlim:
        ax.set_xlim(xlim, auto=True)
    if xticks:
        ax.set_xticks(xticks)
    if ylim:
        ax.set_ylim(ylim, auto=True)
    if yticks:
        ax.set_yticks(yticks)
    if title:
        ax.set_title(title)

    # Set the tick size
    ax.tick_params(axis="both", which="major", labelsize=ticksize)
    ax.tick_params(axis="both", which="minor", labelsize=ticksize)

    if save_dir is not None:
        plt.savefig(save_dir, transparent=transparent, bbox_inches=bbox_inches)
        print(f"Figure saved to {save_dir}")

    # Show plot
    if show:
        plt.show()
